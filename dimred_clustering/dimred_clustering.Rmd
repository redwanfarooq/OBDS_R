---
title: "Example code for dimensionality reduction and clustering in R"
author: "Kevin Rue-Albrecht"
date: "03/10/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load packages
library(tidyverse)
library(ggplot2)
library(cowplot)
library(umap)
library(Rtsne)
library(dendextend)
library(dbscan)
```

# Exercise

## Setup

-   Import the `iris` data set.

```{r}
iris
```

-   Separate the matrix of measurements in a new object named `iris_features`.

```{r}
iris_features <- iris %>% 
  select(where(is.numeric)) %>% 
  as.matrix()
head(iris_features)
```

# Exercise

## Apply Principal Components Analysis (PCA)

The `prcomp()` function allows you to standardise the data as part of the principal components analysis itself.

-   Apply PCA while centering and scaling the matrix of features.

```{r}
pca_iris <- prcomp(iris_features, center = TRUE, scale. = TRUE)
pca_iris
```

-   Examine the PCA output. Display the loading of each feature on each principal component.

```{r}
pca_iris$rotation
```

```{r}
pca_iris$sdev
```

```{r}
str(pca_iris)
```

-   Use the return value of the PCA to create a `data.frame` called `pca_iris_dataframe` that contains the coordinates projected on principal components.

```{r}
pca_iris_dataframe <- as.data.frame(pca_iris$x)
head(pca_iris_dataframe)
```

-   Visualise the PCA projection using `ggplot2::geom_point()`.

```{r}
ggplot(pca_iris_dataframe, aes(x = PC1, y = PC2)) +
  geom_point() +
  theme_cowplot()
```

### Bonus point

-   Color data points according to their class label.

-   Store the PCA plot as an object named `pca_iris_species`.

```{r}
pca_iris_dataframe <- pca_iris_dataframe %>% 
  bind_cols(Species = iris$Species)
head(pca_iris_dataframe)
```

```{r}
pca_iris_species <- ggplot(pca_iris_dataframe, aes(x = PC1, y = PC2, colour = Species)) +
  geom_point() +
  theme_cowplot()
pca_iris_species
```

# Exercise

## Variable loading

-   Color a scatter plot of PC1 and PC2 by the value of the variable most strongly associated with the first principal component.

What do you observe?

```{r}
pca_iris_dataframe <- pca_iris_dataframe %>%
  bind_cols(
    Sepal.Length = iris$Sepal.Length,
    Sepal.Width = iris$Sepal.Width,
    Petal.Length = iris$Petal.Length,
    Petal.Width = iris$Petal.Width
    )
```

```{r}
ggplot(pca_iris_dataframe, aes(x = PC1, y = PC2, colour = Petal.Length)) +
  geom_point() +
  theme_cowplot()
```

> Answer: Directional gradient of variable with PC1

## Variance explained

-   Compute the variance explained by principal components, using information present in the return value of the `prcomp()` function.

```{r}
explained_variance_ratio <- (pca_iris$sdev ^ 2) / sum(pca_iris$sdev ^ 2)
explained_variance_ratio
```

-   Visualise the variance explained by each principal component using `ggplot2::geom_col()`.

```{r}
pca_iris
```

```{r}
variance_dataframe <- data.frame(
  PC = dimnames(pca_iris$x)[[2]],
  Variance = explained_variance_ratio
)
head(variance_dataframe)
```

```{r}
ggplot(variance_dataframe, aes(x = PC, y = Variance)) +
  geom_col(fill = "grey", colour = "black") +
  theme_cowplot()
```

# Exercise

## UMAP

-   Apply UMAP on the output of the PCA.

    ```{r}
    pca_iris$x
    ```

```{r}
set.seed(1) # Set a seed for reproducible results
umap_iris <- umap(pca_iris$x)
umap_iris
```

-   Inspect the UMAP output.

```{r}
str(umap_iris)
```

```{r}
umap_iris$layout
```

-   Visualise the UMAP projection using `ggplot2::geom_point()`.

```{r}
umap_iris_dataframe <- data.frame(
  x = umap_iris$layout[, 1],
  y = umap_iris$layout[, 2]
)

head(umap_iris_dataframe)
```

```{r}
ggplot(umap_iris_dataframe, aes(x = x, y = y)) +
  geom_point() +
  labs(
    x = "UMAP1",
    y = "UMAP2"
  ) +
  theme_cowplot()
```

### Bonus point

-   Color data points according to their class label.

-   Store the UMAP plot as an object named `umap_iris_species`.

```{r}
umap_iris_dataframe <- umap_iris_dataframe %>% 
  bind_cols(Species = iris$Species)
head(umap_iris_dataframe)
```

```{r}
umap_iris_species <- ggplot(umap_iris_dataframe, aes(x = x, y = y, colour = Species)) +
  geom_point() +
  labs(
    x = "UMAP1",
    y = "UMAP2"
  ) +
  theme_cowplot()
umap_iris_species
```

# Exercise

## t-SNE

-   Apply t-SNE and inspect the output.

```{r}
set.seed(1) # Set a seed for reproducible results
tsne_iris <- Rtsne(pca_iris$x, pca = FALSE, check_duplicates = FALSE)
str(tsne_iris)
```

```{r}
tsne_iris$Y
```

-   Use the return value of the t-SNE to create a `data.frame` called `tsne_iris_dataframe` that contains the coordinates.

```{r}
tsne_iris_dataframe <- data.frame(
  x = tsne_iris$Y[, 1],
  y = tsne_iris$Y[, 2]
)

head(tsne_iris_dataframe)
```

-   Visualise the t-SNE projection.

```{r}
ggplot(tsne_iris_dataframe, aes(x = x, y = y)) +
  geom_point() +
  labs(
    x = "tSNE1",
    y = "tSNE2"
  ) +
  theme_cowplot()
```

### Bonus points

-   Color data points according to their class label.

-   Store the t-SNE plot as an object named `tsne_iris_species`.

```{r}
tsne_iris_dataframe <- tsne_iris_dataframe %>% 
  bind_cols(Species = iris$Species)
head(tsne_iris_dataframe)
```

```{r}
tsne_iris_species <- ggplot(tsne_iris_dataframe, aes(x = x, y = y, colour = Species)) +
  geom_point() +
  labs(
    x = "tSNE1",
    y = "tSNE2"
  ) +
  theme_cowplot()
tsne_iris_species
```

-   Combine PCA, UMAP and t-SNE plots in a single figure.

```{r, fig.height=1.5, fig.width=6}
prow <- cowplot::plot_grid(
  pca_iris_species + theme(legend.position = "none"),
  umap_iris_species + theme(legend.position = "none"),
  tsne_iris_species + theme(legend.position = "none"),
  nrow = 1,
  align = "hv"
)

# Extract the legend from 1 plot
legend <- get_legend(
  # Create some space to the left of the legend
  pca_iris_species + theme(legend.box.margin = margin(0, 0, 0, 12))
)

dimred_plots <- cowplot::plot_grid(
  prow,
  legend,
  rel_widths = c(3, 0.5)
)

dimred_plots

ggsave(dimred_plots, filename = "dimred_plots.png", width = 3000, height = 750, units = "px")
```

# Exercise

## Hierarchical clustering

-   Perform hierarchical clustering on the `iris_features` data set, using the `euclidean` distance and method `ward.D2`. Use the functions `dist()` and `hclust()`.

```{r}
dist_iris <- dist(iris_features, method = "euclidean")
hclust_iris_ward <- hclust(dist_iris, method = "ward.D2")
hclust_iris_ward
```

-   Plot the clustering tree. Use the function `plot()`.

```{r, fig.width=6}
plot(hclust_iris_ward)
```

How many clusters would you call from a visual inspection of the tree?

> Answer: 3

-   **Bonus point:** Color leaves by known species (use `dendextend`).

```{r, fig.width=6}
iris_hclust_dend <- as.dendrogram(hclust_iris_ward)
labels_colors(iris_hclust_dend) <- as.numeric(iris$Species)
plot(iris_hclust_dend)
```

-   Cut the tree in 3 clusters and extract the cluster label for each flower. Use the function `cutree()`.

```{r}
iris_hclust_ward_labels <- cutree(hclust_iris_ward, k = 3)
iris_hclust_ward_labels
```

-   Repeat clustering using 3 other agglomeration methods:

    -   `complete`
    -   `average`
    -   `single`

```{r}
# complete
hclust_iris_complete <- hclust(dist_iris, method = "complete")
iris_hclust_complete_labels <- cutree(hclust_iris_complete, k = 3)
iris_hclust_complete_labels
```

```{r}
# average
hclust_iris_average <- hclust(dist_iris, method = "average")
iris_hclust_average_labels <- cutree(hclust_iris_average, k = 3)
iris_hclust_average_labels
```

```{r}
# single
hclust_iris_single <- hclust(dist_iris, method = "single")
iris_hclust_single_labels <- cutree(hclust_iris_single, k = 3)
iris_hclust_single_labels
```

-   Compare clustering results on scatter plots of the data.

```{r}
iris_clusters_dataframe <- data.frame(x = umap_iris$layout[, 1], y = umap_iris$layout[, 2])
iris_clusters_dataframe$hclust_average <- as.factor(iris_hclust_average_labels)
iris_clusters_dataframe$hclust_complete <- as.factor(iris_hclust_complete_labels)
iris_clusters_dataframe$hclust_single <- as.factor(iris_hclust_single_labels)
iris_clusters_dataframe$hclust_ward <- as.factor(iris_hclust_ward_labels)

iris_clusters_dataframe
```

```{r}
plot_average <- ggplot(iris_clusters_dataframe, aes(x = x, y = y, colour = hclust_average)) +
  geom_point() +
  labs(
    x = "UMAP1",
    y = "UMAP2"
  )  +
  theme_cowplot()
  
plot_complete <- ggplot(iris_clusters_dataframe, aes(x = x, y = y, colour = hclust_complete)) +
  geom_point() +
  labs(
    x = "UMAP1",
    y = "UMAP2"
  )  +
  theme_cowplot()

plot_single <- ggplot(iris_clusters_dataframe, aes(x = x, y = y, colour = hclust_single)) +
  geom_point() +
  labs(
    x = "UMAP1",
    y = "UMAP2"
  )  +
  theme_cowplot()


plot_ward <- ggplot(iris_clusters_dataframe, aes(x = x, y = y, colour = hclust_ward)) +
  geom_point() +
  labs(
    x = "UMAP1",
    y = "UMAP2"
  )  +
  theme_cowplot()


cowplot::plot_grid(
  plot_average,
  plot_complete,
  plot_single,
  plot_ward,
  ncol = 2
)
```

# Exercise

## dbscan

-   Apply `dbscan` to the `iris_features` data set.

```{r}
dbscan_iris <- dbscan(iris_features, eps = 0.5) # Epsilon represents radius in density-based clustering algorithm
dbscan_iris
```

-   Visualise the `dbscan` cluster label on a scatter plot of the data.

```{r}
iris_clusters_dataframe$dbscan <- as.factor(dbscan_iris$cluster)
head(iris_clusters_dataframe)
```

```{r}
dbscan_plot <- ggplot(iris_clusters_dataframe, aes(x = x, y = y, colour = dbscan)) +
  geom_point() +
  labs(
    x = "UMAP1",
    y = "UMAP2"
  )  +
  theme_cowplot()

dbscan_plot
```

## hdbscan

-   Apply `hdbscan` to the `iris_features` data set.

```{r}
hdbscan_iris <- hdbscan(iris_features, minPts = 5)
hdbscan_iris
```

-   Visualise the `hdbscan` cluster label on a scatter plot of the data.

```{r}
iris_clusters_dataframe$hdbscan <- as.factor(hdbscan_iris$cluster)
head(iris_clusters_dataframe)
```

```{r}
hdbscan_plot <- ggplot(iris_clusters_dataframe, aes(x = x, y = y, colour = hdbscan)) +
  geom_point() +
  labs(
    x = "UMAP1",
    y = "UMAP2"
  )  +
  theme_cowplot()
  
hdbscan_plot
```

## Bonus point

-   Combine the plots of `dbscan` and `hdbscan` into a single plot.

```{r, fig.width=4, fig.height=1.5}
cowplot::plot_grid(
  dbscan_plot,
  hdbscan_plot,
  nrow = 1
)
```

# Exercise

## K-means clustering

-   Apply $K$-means clustering with $K$ set to 3 clusters.

```{r}
set.seed(1) # Set a seed for reproducible results
kmeans_iris <- kmeans(iris_features, centers = 3)
kmeans_iris
```

-   Inspect the output.

```{r}
str(kmeans_iris)
```

-   Extract the cluster labels.

```{r}
kmeans_iris$cluster
```

-   Extract the coordinates of the cluster centers.

```{r}
kmeans_iris$centers
```

-   Construct a data frame that combines the `iris` dataset (or reduced dimensionality representation) and the cluster label.

```{r}
iris_labelled <- data.frame(x = umap_iris$layout[, 1], y = umap_iris$layout[, 2])
iris_labelled$Kmeans <- as.factor(kmeans_iris$cluster)
iris_labelled$Species <- as.factor(iris$Species)
head(iris_labelled)
```

-   Plot the data set as a scatter plot.

    -   Color by cluster label.

```{r}
ggplot(iris_labelled, aes(x = x, y = y, colour = Kmeans)) +
  geom_point() +
  labs(
    x = "UMAP1",
    y = "UMAP2"
  ) +
  theme_cowplot()
```

### Bonus point

-   Add cluster centers as points in the plot (not possible for reduced dimensionality representation).

```{r}
# iris_means_centers <- as.data.frame()
# iris_means_centers$Kmeans <- as.factor()
# head(iris_means_centers)
```

```{r}
# ggplot(iris_labelled,    ) +
#   
```

# Exercise

## Cross-tabulation with ground truth

-   Cross-tabulate cluster labels with known labels.

```{r}
table(iris$Species, kmeans_iris$cluster)
```

How many observations are mis-classified by $K$-means clustering?

> Answer: 16

## Elbow plot

-   Plot the "total within-cluster sum of squares" for K ranging from 2 to 10.

```{r}
get_kmeans_tot.withinss <- function(k, data) {
  kmeans_out <- kmeans(data, k)
  return(kmeans_out$tot.withinss)
}
```

```{r}
k_range <- 2:10
tot.withinss <- vapply(k_range, FUN = get_kmeans_tot.withinss, data = iris_features, FUN.VALUE = numeric(1))
k_means_dataframe <- data.frame(
  k = k_range,
  tot.withinss = tot.withinss
)
head(k_means_dataframe)
```

```{r}
ggplot(k_means_dataframe, aes(x = k, y = tot.withinss)) +
  geom_point() +
  geom_line(colour = "grey") + 
  theme_cowplot()
```

Do you agree that 3 is the optimal number of clusters for this data set?

> Answer: 5 may be a better choice (i.e. there may be subgroups within species or subgroups which overlap species boundaries).
